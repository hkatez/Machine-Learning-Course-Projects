{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 4:  Multi-class Classification, SVM, Data Leakage. (Due 10/10 11:59pm)\n",
    "\n",
    "For this assignment, question 1 is worth 40 points, and question 2 and 3 are worth 20 points each, for a total of 80 points. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "Submit your completed notebook file AND corresponding **HTML** file to the Canvas site.\n",
    "\n",
    "As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates: if you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your name here: Huan Zhao\n",
    "\n",
    "### Put your uniquename here: huanzhao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (40 points)\n",
    "\n",
    "Please write the answers as well as your derivation process of the following questions. You can use either LaTeX or python code to represent your answer. For example, if you want to present <$x_1^2$>, in the LaTeX format you should write <(dollar sign) x_1^2 (dollar sign)>; in the python code format you should write <\\`x_1\\*\\*2\\`>. See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to represent more mathmatical symbols in LaTeX format.\n",
    "\n",
    "*Note: The whole question 1 does not require coding.*\n",
    "\n",
    "#### (a) (10 points) \n",
    "\n",
    "If you have data with features $(x_1, x_2)$, what will be the set of the expanded features after you apply the `PolynomialFeatures` transformation with `degree=3` on it? The order of the features does not matter in your answer.\n",
    "\n",
    "#### (b) (10 points) \n",
    "The main metric we have been used to measure the quality of regression models is $R^2$, which is defined as, for n data points, $R^2 = 1 -  \\frac{\\sum_{i=1}^n(\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$, where $y_i, \\hat{y}_i$ are the label and prediction of data point i, and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$. We denote $\\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i - y_i)^2$ as *Unexplained Variation* and $\\frac{1}{n}\\sum_{i=1}^n(y_i - \\bar{y})^2$ as *Total Variation*. \n",
    "\n",
    "Given 5 data points with labels (1, 3, 2, -4, 6) and two classifiers A and B, the predictions of A is (1.1, 1.4, 1.3, -2, 2) and the predictions of B is (1.7, 1.3, 0.3, 2, 3). Please calculate and report the *Unexplained Variation*, *Total Variation*, and *$R^2$* for classifier A and B respectively. \n",
    "\n",
    "#### (c) (20 points) \n",
    "You are given 3 data points with two features and one labels as follows,\n",
    "\n",
    "|    X1\t| X2 \t| Y \t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1\t|   1 \t| 1.05 \t|\n",
    "|   0.7 |  3 \t| 0.81 \t|\n",
    "|   2   |  0.5 \t| 2.045 |\n",
    "\n",
    "Suppose you have a linear regression model: $y = w_1 x_1 + w_2 x_2$, please calculate and report the least square error, the L1 regularization, and the L2 regularization terms when\n",
    "\n",
    "(i) $w_1 = 1, w_2 = 0$\n",
    "\n",
    "(ii) $w_1 = 1, w_2 = 0.02$\n",
    "\n",
    "If you set the regularization coefficient $C=1$, which of the above two weights is preferred by the Lasso and which is preferred by Ridge regression? Could you use this example to explain why Lasso prefers sparse models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1(a)\n",
    "\n",
    "The set of expanded features would be\n",
    "\n",
    "1,\n",
    "x1,\n",
    "x2,\n",
    "$x_1^2$,\n",
    "x1x2,\n",
    "$x_2^2$,\n",
    "$x_1^3$,\n",
    "$x_1^2$x2,\n",
    "x1$x_2^2$,\n",
    "$x_2^3$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1(b)\n",
    "\n",
    "Since we have 5 data points with labels (1, 3, 2, -4, 6) and two classifiers A and B, the predictions of A is (1.1, 1.4, 1.3, -2, 2) and the predictions of B is (1.7, 1.3, 0.3, 2, 3)\n",
    "\n",
    "\n",
    "$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$=$\\frac{1}{5}\\cdot(1+3+2-4+6)=1.6$  \n",
    "$\\bar{y_A} = \\frac{1}{5}\\sum_{i=1}^5 y_iA$= $\\frac{1}{5}\\cdot(1.1+1.4+1.3-2+2)=0.76$  \n",
    "$\\bar{y_B} = \\frac{1}{5}\\sum_{i=1}^5 y_iB$  = $\\frac{1}{5}\\cdot(1.7+1.3+0.3+2+3)= 1.66$\n",
    "\n",
    "Since Unexplained Variation = $\\frac{1}{n}\\sum_{i=1}^n(\\hat{y}_i - y_i)^2$  \n",
    "\n",
    "Then Unexplained Variation for A = $\\frac{1}{5}\\sum_{i=1}^5(\\hat{y_A}_i - y_i)^2$ = $\\frac{1}{5}\\cdot[(1.1-1)^2+(1.4-3)^2+(1.3-2)^2+(-2-(-4))^2+(2-6)^2$] = $\\frac{1}{5}\\cdot(23.06)=4.612$  \n",
    "\n",
    "Unexplained Variation for B = $\\frac{1}{5}\\sum_{i=1}^5(\\hat{y_B}_i - y_i)^2$ = $\\frac{1}{5}\\cdot[(1.7-1)^2+(1.3-3)^2+(0.3-2)^2+(2-(-4))^2+(3-6)^2$] = $\\frac{1}{5}\\cdot(51.27)=10.254$  \n",
    "\n",
    "\n",
    "Total Variation = $\\frac{1}{n}\\sum_{i=1}^n(y_i - \\bar{y})^2$ = $\\frac{1}{5}\\sum_{i=1}^5(y_i - \\bar{y})^2$ = $\\frac{1}{5}[(1-1.6)^2+(3-1.6)^2+(2-1.6)^2+(-4-1.6)^2+(6-1.6)^2]=10.64$  \n",
    "\n",
    "Since $R^2 = 1 -  \\frac{\\sum_{i=1}^n(\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$  \n",
    "Then $R^2$ for A = $1 -  \\frac{\\sum_{i=1}^n(\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}=1-(4.617/10.64)=0.566$  \n",
    "$R^2$ for B = $1 -  \\frac{\\sum_{i=1}^n(\\hat{y}_i - y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}=1-(10.254/10.64)=0.036$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 1(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since least square error = ${}\\sum_{i=0}^n(\\epsilon_i)^2$ = ${}\\sum_{i=0}^n(y_i-(\\alpha+\\beta x_i))^2$\n",
    "\n",
    "(i) ùë§1=1,ùë§2=0\n",
    "(ii) ùë§1=1,ùë§2=0.02\n",
    "\n",
    "We have the regression model for i as $\\hat{y}$ = $x_1$\n",
    "\n",
    "The regression model for ii as $\\hat{y}$ = $x_1+0.02x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least square error of i:  0.01662500000000002\n",
      "Least square error of ii:  0.004625000000000016\n"
     ]
    }
   ],
   "source": [
    "X1 = (1,0.7,2)\n",
    "X2 = (1,3,0.5)\n",
    "Y = (1.05,0.81,2.045)\n",
    "Wi = (1,0)\n",
    "Wii = (1,0.02)\n",
    "\n",
    "def lse(X1, X2, W, Y):\n",
    "    lse = 0\n",
    "    for i in range(3):\n",
    "        f = W[0] * X1[i] + W[1] * X2[i]\n",
    "        a = (Y[i] - f) * (Y[i] - f)\n",
    "        lse += a\n",
    "    return lse\n",
    "print(\"Least square error of i:\", lse(X1,X2,Wi,Y))\n",
    "print(\"Least square error of ii: \", lse(X1,X2,Wii,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \n",
    "\n",
    "L1 regularization = |$w_1$| + |$w2$|  \n",
    "L2 regularization = $(w_1)^2$ + $(w2)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 regularization of i:  1\n",
      "L2 regularization of i:  1\n",
      "L1 regularization of ii:  1.02\n",
      "L2 regularization of ii:  1.0004\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "L1_i = reduce(lambda x, y: abs(x) + abs(y),Wi)\n",
    "L2_i = reduce(lambda x, y: x**2 + y**2,Wi)\n",
    "\n",
    "L1_ii = reduce(lambda x, y: abs(x) + abs(y),Wii)\n",
    "L2_ii = reduce(lambda x, y: x**2 + y**2,Wii)\n",
    "\n",
    "print('L1 regularization of i: ', L1_i)\n",
    "print('L2 regularization of i: ', L2_i)\n",
    "\n",
    "print('L1 regularization of ii: ', L1_ii)\n",
    "print('L2 regularization of ii: ', L2_ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When C = 1, i is preferred by the Lasso and ii is preferred by Ridge regression. \n",
    "\n",
    "For L2: Penalty term is squared,so squaring a small value will make it smaller. We don't have to make it zero to achieve our aim to get minimum square error, we will get it before that. For L1: Penalty term is absolute,we might need to go to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (20 points)\n",
    "\n",
    "First use `MinMaxScaler` to scale the breast cancer data and then use `GridSearchCV` to search the `kernel`, `C`, and `gamma` parameters for `SVC`. Be careful about the data leakage issues. Please return the best hyper-parameters on cross-validation and the test score associated with the these hyper-parameters.\n",
    "\n",
    "Please search the `kernel` from ('linear', 'rbf'), `C` from (0.1, 1, 10, 100), `gamma` from (0.1, 1, 10, 100). And please apply `random_state=0` in both `train_test_split`.\n",
    "\n",
    "*This function should a return a tuple with four numbers, i.e. `(best_kernel, best_C, best_gamma, test_score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rbf', 1, 1, 0.9812206572769953)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # load the cancer dataset\n",
    "    (X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n",
    "                                                   random_state = 0)\n",
    "    # scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # SVC\n",
    "    tuned_parameters = {'kernel': ['linear', 'rbf'], 'gamma': [0.1, 1, 10, 100],'C': [0.1, 1, 10, 100]}\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_kernel = clf.best_params_['kernel']    \n",
    "    best_C = clf.best_params_['C']\n",
    "    best_gamma = clf.best_params_['gamma']\n",
    "    test_score = clf.best_score_\n",
    "    \n",
    "    return best_kernel, best_C, best_gamma, test_score\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (20 points)\n",
    "\n",
    "Suppose you have a dataset with some missing values and you know the values are not missing at random and the probability of missing is related to the values themselves. For example, people with higher\n",
    "earnings may be less likely to reveal them. \n",
    "\n",
    "#### (a) (3 points) In this case, what would happen when imputing the missing values with the mean strategy?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 3(a)\n",
    "\n",
    "Imputing the missing values with the mean strategy will add bias, especially when the values are not missing at random and the probability of missing is related to the values themselves. If we add a missing value based on the mean of the existing earnings, it would probably be lower than the acutal value, which make our dataset inaccurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance based on the full data: 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huan Zhao\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Please run this cell first before the question (b)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "d = {}\n",
    "for i in range(len(cancer.feature_names)):\n",
    "    d[cancer.feature_names[i]] = cancer.data[:, i]\n",
    "d['target'] = cancer.target\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "X = df[['mean concave points', 'worst concave points']]\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), random_state=0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('The performance based on the full data: {:.3f}'.format(lr.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "X_train_missing = X_train.copy()\n",
    "u = rng.uniform(low=0.3, high=1, size=(n_samples,))\n",
    "X_train_missing[np.where(u < X_train[:, 0])[0], 0] = np.nan\n",
    "u = rng.uniform(low=0.3, high=1, size=(n_samples,))\n",
    "X_train_missing[np.where(u < X_train[:, 1])[0], 1] = np.nan\n",
    "\n",
    "n_samples = X_test.shape[0]\n",
    "X_test_missing = X_test.copy()\n",
    "u = rng.uniform(low=0.3, high=1, size=(n_samples,))\n",
    "X_test_missing[np.where(u < X_test[:, 0])[0], 0] = np.nan\n",
    "u = rng.uniform(low=0.3, high=1, size=(n_samples,))\n",
    "X_test_missing[np.where(u < X_test[:, 1])[0], 1] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) (7 points) \n",
    "\n",
    "Please impute the missing values using `SimpleImputer` with `strategy='mean'`. Then fit a LogisticRegression with default hyper-parameters, and return the imputed data and the test score.\n",
    "\n",
    "\n",
    "*This function should a return a tuple of two arrays and one number: `(X_train_imputed, X_test_imputed, test_score)`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huan Zhao\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.11332008, 0.31484671],\n",
       "        [0.03768887, 0.06965208],\n",
       "        [0.48663022, 0.56079917],\n",
       "        [0.06153082, 0.17860834],\n",
       "        [0.11466203, 0.30096452],\n",
       "        [0.04218688, 0.1754392 ],\n",
       "        [0.10377734, 0.19138822],\n",
       "        [0.21824056, 0.3191526 ],\n",
       "        [0.38757455, 0.51842921],\n",
       "        [0.21744533, 0.33000244],\n",
       "        [0.15666004, 0.49156045],\n",
       "        [0.09701789, 0.19249053],\n",
       "        [0.44875746, 0.55597658],\n",
       "        [0.14632207, 0.25673441],\n",
       "        [0.15333002, 0.45780227],\n",
       "        [0.09120278, 0.12442301],\n",
       "        [0.21115665, 0.73062349],\n",
       "        [0.54821074, 0.33000244],\n",
       "        [0.04242048, 0.13720289],\n",
       "        [0.17957256, 0.53565277],\n",
       "        [0.28508946, 0.59903548],\n",
       "        [0.14289264, 0.29197382],\n",
       "        [0.5417495 , 0.87599035],\n",
       "        [0.29746521, 0.53599724],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.18802187, 0.44230107],\n",
       "        [0.12047714, 0.19338615],\n",
       "        [0.06371769, 0.08832243],\n",
       "        [0.09786282, 0.25597658],\n",
       "        [0.08846918, 0.22487082],\n",
       "        [0.11585487, 0.28284533],\n",
       "        [0.2889662 , 0.33000244],\n",
       "        [0.        , 0.        ],\n",
       "        [0.03944831, 0.0820186 ],\n",
       "        [0.15457256, 0.2508784 ],\n",
       "        [0.0887674 , 0.18966586],\n",
       "        [0.16570577, 0.52704099],\n",
       "        [0.61829026, 0.63658285],\n",
       "        [0.10720676, 0.27971064],\n",
       "        [0.16819085, 0.33000244],\n",
       "        [0.22311133, 0.40785394],\n",
       "        [0.10427435, 0.20151567],\n",
       "        [0.09319085, 0.10764726],\n",
       "        [0.17534791, 0.43093352],\n",
       "        [0.16297217, 0.28973476],\n",
       "        [0.44701789, 0.72028936],\n",
       "        [0.03914016, 0.11756803],\n",
       "        [0.06411531, 0.08883913],\n",
       "        [0.36779324, 0.66551843],\n",
       "        [0.46486083, 0.70961075],\n",
       "        [0.07808151, 0.27440579],\n",
       "        [0.19706759, 0.33565277],\n",
       "        [0.48409543, 0.86358939],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.05492048, 0.1712022 ],\n",
       "        [0.18543738, 0.31767137],\n",
       "        [0.14408549, 0.09986221],\n",
       "        [0.10387674, 0.21991044],\n",
       "        [0.09781312, 0.23065794],\n",
       "        [0.14100398, 0.28367206],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.21115665, 0.77574922],\n",
       "        [0.01656561, 0.03827075],\n",
       "        [0.10074553, 0.30413365],\n",
       "        [0.34980119, 0.37030658],\n",
       "        [0.55715706, 0.33000244],\n",
       "        [0.13916501, 0.30926628],\n",
       "        [0.17311133, 0.33489494],\n",
       "        [0.02510437, 0.06379607],\n",
       "        [0.03876243, 0.15222184],\n",
       "        [0.0146173 , 0.05742336],\n",
       "        [0.49850895, 0.33000244],\n",
       "        [0.13856859, 0.33968309],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.32281312, 0.36823975],\n",
       "        [0.21115665, 0.52635205],\n",
       "        [0.12281312, 0.36307268],\n",
       "        [0.34100398, 0.68101963],\n",
       "        [0.11322068, 0.21756803],\n",
       "        [0.17087475, 0.39958663],\n",
       "        [0.11292247, 0.33933862],\n",
       "        [0.11650099, 0.21033414],\n",
       "        [0.09756461, 0.24199104],\n",
       "        [0.1083002 , 0.28463658],\n",
       "        [0.38528827, 0.33000244],\n",
       "        [0.        , 0.        ],\n",
       "        [0.47206759, 0.33000244],\n",
       "        [0.22678926, 0.46744747],\n",
       "        [0.16898608, 0.39786428],\n",
       "        [0.08563618, 0.295763  ],\n",
       "        [0.32987078, 0.33000244],\n",
       "        [0.04055666, 0.11243541],\n",
       "        [0.06704771, 0.21687909],\n",
       "        [0.11292247, 0.29831209],\n",
       "        [0.43653082, 0.33000244],\n",
       "        [0.12837972, 0.14832931],\n",
       "        [0.13161034, 0.33000244],\n",
       "        [0.21115665, 0.5077506 ],\n",
       "        [0.51838966, 0.33000244],\n",
       "        [0.13185885, 0.32834998],\n",
       "        [0.74353877, 0.33000244],\n",
       "        [0.15258449, 0.23203583],\n",
       "        [0.31645129, 0.33000244],\n",
       "        [0.02564115, 0.08883913],\n",
       "        [0.45695825, 0.33000244],\n",
       "        [0.40203777, 0.33000244],\n",
       "        [0.29537773, 0.41887702],\n",
       "        [0.39666998, 0.36066138],\n",
       "        [0.20412525, 0.37202894],\n",
       "        [0.13439364, 0.32142611],\n",
       "        [0.0145328 , 0.03021702],\n",
       "        [0.21115665, 0.77437134],\n",
       "        [0.30869781, 0.50017224],\n",
       "        [0.48489066, 0.82431967],\n",
       "        [0.12738569, 0.27750603],\n",
       "        [0.46873757, 0.5556321 ],\n",
       "        [0.13036779, 0.30857733],\n",
       "        [0.3472664 , 0.58835687],\n",
       "        [0.59343936, 0.70961075],\n",
       "        [0.14264414, 0.33000244],\n",
       "        [0.11645129, 0.22366517],\n",
       "        [0.04778827, 0.13248364],\n",
       "        [0.18578529, 0.2724423 ],\n",
       "        [0.        , 0.        ],\n",
       "        [0.21908549, 0.28312091],\n",
       "        [0.16848907, 0.35032725],\n",
       "        [0.03192346, 0.09590079],\n",
       "        [0.11814115, 0.28367206],\n",
       "        [0.14234592, 0.16441612],\n",
       "        [0.11471173, 0.13248364],\n",
       "        [0.0665507 , 0.21433   ],\n",
       "        [0.06809145, 0.13930417],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.17321074, 0.41956597],\n",
       "        [0.11878728, 0.28894247],\n",
       "        [0.1083499 , 0.17523252],\n",
       "        [0.47718688, 0.68411988],\n",
       "        [0.12664016, 0.28973476],\n",
       "        [0.1360835 , 0.3143989 ],\n",
       "        [0.1028827 , 0.28074406],\n",
       "        [0.16153082, 0.27089218],\n",
       "        [0.13817097, 0.2725112 ],\n",
       "        [0.16158052, 0.2725112 ],\n",
       "        [0.77634195, 0.83430934],\n",
       "        [0.10308151, 0.23265587],\n",
       "        [0.33658052, 0.33000244],\n",
       "        [0.14925447, 0.20812952],\n",
       "        [0.13146123, 0.21687909],\n",
       "        [0.27032803, 0.77781605],\n",
       "        [0.12032803, 0.25015501],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.41515905, 0.63968309],\n",
       "        [0.02867296, 0.08611781],\n",
       "        [0.33593439, 0.52394075],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.22122266, 0.31312435],\n",
       "        [0.09343936, 0.29982776],\n",
       "        [0.36853877, 0.33000244],\n",
       "        [0.10800199, 0.21367551],\n",
       "        [0.05506958, 0.13957975],\n",
       "        [0.16605368, 0.33000244],\n",
       "        [0.43459245, 0.33000244],\n",
       "        [0.09562624, 0.33000244],\n",
       "        [0.09329026, 0.20024113],\n",
       "        [0.08141153, 0.18449879],\n",
       "        [0.18469185, 0.341371  ],\n",
       "        [0.20765408, 0.28732346],\n",
       "        [0.51838966, 0.592835  ],\n",
       "        [0.38692843, 0.33000244],\n",
       "        [0.06779324, 0.17223562],\n",
       "        [0.10765408, 0.27860834],\n",
       "        [0.11023857, 0.32070272],\n",
       "        [0.12922465, 0.24002756],\n",
       "        [0.31083499, 0.52876335],\n",
       "        [0.42698807, 0.33000244],\n",
       "        [0.08851889, 0.28632449],\n",
       "        [0.12594433, 0.31484671],\n",
       "        [0.06242545, 0.14450568],\n",
       "        [0.09666998, 0.24512573],\n",
       "        [0.26073559, 0.47330348],\n",
       "        [0.03197813, 0.10341027],\n",
       "        [0.29488072, 0.33000244],\n",
       "        [0.37738569, 0.61488116],\n",
       "        [0.1861332 , 0.41508784],\n",
       "        [0.32813121, 0.35170513],\n",
       "        [0.2610338 , 0.33000244],\n",
       "        [0.04601889, 0.09569411],\n",
       "        [0.2972167 , 0.33000244],\n",
       "        [0.26824056, 0.33000244],\n",
       "        [0.12142147, 0.24712367],\n",
       "        [0.22067594, 0.34791595],\n",
       "        [0.09776342, 0.15807785],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.6361829 , 0.66241819],\n",
       "        [0.17470179, 0.24664141],\n",
       "        [0.25193837, 0.47640372],\n",
       "        [0.09527833, 0.19986221],\n",
       "        [0.29910537, 0.62452635],\n",
       "        [0.05710736, 0.09231829],\n",
       "        [0.39527833, 0.50155012],\n",
       "        [0.31963221, 0.53565277],\n",
       "        [0.1139662 , 0.20289356],\n",
       "        [0.21242545, 0.41405443],\n",
       "        [0.04388171, 0.12166724],\n",
       "        [0.19333996, 0.33000244],\n",
       "        [0.05323062, 0.12301068],\n",
       "        [0.38424453, 0.51326214],\n",
       "        [0.08633201, 0.19162935],\n",
       "        [0.29527833, 0.50465036],\n",
       "        [0.52385686, 0.52187392],\n",
       "        [0.07450298, 0.22383741],\n",
       "        [0.15298211, 0.33699621],\n",
       "        [0.42862823, 0.57423355],\n",
       "        [0.21115665, 0.54943162],\n",
       "        [0.05969185, 0.11033414],\n",
       "        [0.11232604, 0.2861867 ],\n",
       "        [0.05521869, 0.07654151],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.27842942, 0.33000244],\n",
       "        [0.25218688, 0.33000244],\n",
       "        [0.08220676, 0.1658629 ],\n",
       "        [0.32117296, 0.61040303],\n",
       "        [0.12007952, 0.22487082],\n",
       "        [0.18991054, 0.34963831],\n",
       "        [0.08285288, 0.24877713],\n",
       "        [0.41744533, 0.69376507],\n",
       "        [0.11083499, 0.23978643],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.15616302, 0.39200827],\n",
       "        [0.527833  , 0.33000244],\n",
       "        [0.32117296, 0.50947296],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.50447316, 0.5556321 ],\n",
       "        [0.29925447, 0.38064072],\n",
       "        [0.08439364, 0.15856011],\n",
       "        [0.10646123, 0.21319325],\n",
       "        [0.21212724, 0.32772993],\n",
       "        [0.10094433, 0.38305202],\n",
       "        [0.23578529, 0.39131932],\n",
       "        [0.16222664, 0.26741302],\n",
       "        [0.11505964, 0.2730279 ],\n",
       "        [0.09358847, 0.22762659],\n",
       "        [0.10208748, 0.16510506],\n",
       "        [0.02779324, 0.09631416],\n",
       "        [0.2666004 , 0.38546331],\n",
       "        [0.0582008 , 0.18484327],\n",
       "        [0.13583499, 0.20289356],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.102833  , 0.22542198],\n",
       "        [0.10119284, 0.40647606],\n",
       "        [0.45641153, 0.57285567],\n",
       "        [0.4804672 , 0.61212539],\n",
       "        [0.73011928, 0.33000244],\n",
       "        [0.44348907, 0.33000244],\n",
       "        [0.10094433, 0.24068205],\n",
       "        [0.09279324, 0.19293834],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.08836978, 0.18374096],\n",
       "        [0.06605368, 0.20733724],\n",
       "        [0.39050696, 0.54116431],\n",
       "        [0.26138171, 0.55425422],\n",
       "        [0.32122266, 0.62934895],\n",
       "        [0.14662028, 0.33957975],\n",
       "        [0.0943837 , 0.28515329],\n",
       "        [0.07524851, 0.22597313],\n",
       "        [0.11858847, 0.30141233],\n",
       "        [0.23836978, 0.29662418],\n",
       "        [0.29930417, 0.49776094],\n",
       "        [0.21115665, 0.58456769],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.10024851, 0.23544609],\n",
       "        [0.39512922, 0.33000244],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.21615308, 0.31643128],\n",
       "        [0.08026839, 0.21136755],\n",
       "        [0.10437376, 0.29962108],\n",
       "        [0.22350895, 0.33000244],\n",
       "        [0.11774354, 0.33000244],\n",
       "        [0.23762425, 0.44367895],\n",
       "        [0.42186879, 0.33000244],\n",
       "        [0.06804175, 0.11002411],\n",
       "        [0.15705765, 0.31267654],\n",
       "        [0.14249503, 0.29624526],\n",
       "        [0.24030815, 0.31426111],\n",
       "        [0.06043738, 0.15377196],\n",
       "        [0.24607356, 0.35032725],\n",
       "        [0.52286282, 0.33000244],\n",
       "        [0.41217694, 0.33000244],\n",
       "        [0.05670974, 0.17581812],\n",
       "        [0.48031809, 0.33000244],\n",
       "        [0.18841948, 0.39786428],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.16217694, 0.42232174],\n",
       "        [0.13205765, 0.28170858],\n",
       "        [0.24085487, 0.4622804 ],\n",
       "        [0.0554672 , 0.07688598],\n",
       "        [0.36799205, 0.30816397],\n",
       "        [0.09562624, 0.21849811],\n",
       "        [0.65705765, 0.33000244],\n",
       "        [0.25790258, 0.53083018],\n",
       "        [0.59791252, 0.63623837],\n",
       "        [0.1109841 , 0.21105753],\n",
       "        [0.01725646, 0.04784705],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.32833002, 0.62349294],\n",
       "        [0.0270825 , 0.05632105],\n",
       "        [0.15198807, 0.46124699],\n",
       "        [0.24542744, 0.42197726],\n",
       "        [0.13131213, 0.2730279 ],\n",
       "        [0.27823062, 0.49844988],\n",
       "        [0.22673956, 0.33000244],\n",
       "        [0.02071074, 0.03827075],\n",
       "        [0.28175944, 0.33000244],\n",
       "        [0.03106362, 0.11481226],\n",
       "        [0.12326044, 0.25597658],\n",
       "        [0.0554175 , 0.12797106],\n",
       "        [0.15690855, 0.33000244],\n",
       "        [0.12629225, 0.27402687],\n",
       "        [0.12892644, 0.2383052 ],\n",
       "        [0.02815109, 0.09755425],\n",
       "        [0.21115665, 0.85256631],\n",
       "        [0.21115665, 0.50499483],\n",
       "        [0.28717694, 0.42059938],\n",
       "        [0.09517893, 0.19204271],\n",
       "        [0.05347913, 0.14832931],\n",
       "        [0.43006958, 0.61488116],\n",
       "        [0.        , 0.        ],\n",
       "        [0.0555169 , 0.19913882],\n",
       "        [0.14324056, 0.2820186 ],\n",
       "        [0.1609841 , 0.37857389],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.07589463, 0.19600413],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.30606362, 0.51395109],\n",
       "        [0.4360338 , 0.77437134],\n",
       "        [0.16530815, 0.21584568],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.14314115, 0.3377196 ],\n",
       "        [0.372167  , 0.66792973],\n",
       "        [0.14572565, 0.27168446],\n",
       "        [0.42072565, 0.62693765],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.13906561, 0.25466759],\n",
       "        [0.16650099, 0.29104375],\n",
       "        [0.04228131, 0.1562866 ],\n",
       "        [0.0888171 , 0.16417499],\n",
       "        [0.07465209, 0.1897692 ],\n",
       "        [0.1193837 , 0.28329315],\n",
       "        [0.16749503, 0.36376163],\n",
       "        [0.00920477, 0.03189459],\n",
       "        [0.74602386, 0.33000244],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.43812127, 0.72855667],\n",
       "        [0.28717694, 0.31625904],\n",
       "        [0.43156064, 0.33000244],\n",
       "        [0.18568588, 0.38064072],\n",
       "        [0.14025845, 0.28225973],\n",
       "        [0.18792247, 0.29228384],\n",
       "        [0.14234592, 0.20261798],\n",
       "        [0.06525845, 0.1658629 ],\n",
       "        [0.13663022, 0.22555977],\n",
       "        [0.1139165 , 0.27971064],\n",
       "        [0.4443837 , 0.33000244],\n",
       "        [0.20293241, 0.28577334],\n",
       "        [0.26789264, 0.37754048],\n",
       "        [0.18444334, 0.34423011],\n",
       "        [0.32877734, 0.34963831],\n",
       "        [0.18787276, 0.33000244],\n",
       "        [0.2749503 , 0.55080951],\n",
       "        [0.15347913, 0.34860489],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.40193837, 0.33000244],\n",
       "        [0.36481113, 0.33000244],\n",
       "        [0.08409543, 0.19138822],\n",
       "        [0.45606362, 0.33000244],\n",
       "        [0.09259443, 0.26290045],\n",
       "        [0.03601392, 0.19138822],\n",
       "        [0.37191849, 0.50843955],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.48265408, 0.33000244],\n",
       "        [0.27813121, 0.40819842],\n",
       "        [0.23951292, 0.51291767],\n",
       "        [0.40372763, 0.63176025],\n",
       "        [0.38399602, 0.49087151],\n",
       "        [0.61729622, 0.33000244],\n",
       "        [0.04938867, 0.11408887],\n",
       "        [0.49507952, 0.33000244],\n",
       "        [0.13702783, 0.28494661],\n",
       "        [0.49373757, 0.33000244],\n",
       "        [0.06993042, 0.17771271],\n",
       "        [0.155666  , 0.33809852],\n",
       "        [0.52683897, 0.45642439],\n",
       "        [0.30511928, 0.46916982],\n",
       "        [0.42415507, 0.49018257],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.11734592, 0.21687909],\n",
       "        [0.05183897, 0.1676197 ],\n",
       "        [0.13116302, 0.33000244],\n",
       "        [0.10775348, 0.27140889],\n",
       "        [0.21115665, 0.53875301],\n",
       "        [0.16277336, 0.27413021],\n",
       "        [0.11774354, 0.23579056],\n",
       "        [0.21115665, 0.57147778],\n",
       "        [0.2777336 , 0.32084051],\n",
       "        [0.23916501, 0.48467103],\n",
       "        [0.17703777, 0.28952807],\n",
       "        [0.44065606, 0.70099897],\n",
       "        [0.29020875, 0.53909749],\n",
       "        [0.14537773, 0.3304857 ],\n",
       "        [0.13911531, 0.2538753 ],\n",
       "        [0.09980119, 0.24963831],\n",
       "        [0.39483101, 0.33000244],\n",
       "        [0.22519881, 0.45056838],\n",
       "        [0.        , 0.        ],\n",
       "        [0.4110835 , 0.33000244],\n",
       "        [0.11217694, 0.2209094 ],\n",
       "        [0.13871769, 0.26965208],\n",
       "        [0.        , 0.        ],\n",
       "        [0.26744533, 0.50051671],\n",
       "        [0.16008946, 0.39510851],\n",
       "        [0.39517893, 0.61625904],\n",
       "        [0.29040755, 0.44574578],\n",
       "        [0.42460239, 0.33000244],\n",
       "        [0.06988072, 0.17402687],\n",
       "        [0.        , 0.        ],\n",
       "        [0.20402584, 0.33251808]]), array([[0.21115665, 0.33000244],\n",
       "        [0.1027833 , 0.20685498],\n",
       "        [0.13180915, 0.28301757],\n",
       "        [0.06829026, 0.25938684],\n",
       "        [0.14527833, 0.17268343],\n",
       "        [0.12892644, 0.29424733],\n",
       "        [0.14801193, 0.33000244],\n",
       "        [0.05705765, 0.19776094],\n",
       "        [0.        , 0.        ],\n",
       "        [0.10124254, 0.14033758],\n",
       "        [0.31312127, 0.38167413],\n",
       "        [0.14696819, 0.23926972],\n",
       "        [0.02548708, 0.08832243],\n",
       "        [0.13772366, 0.29059594],\n",
       "        [0.35109344, 0.48122632],\n",
       "        [0.21115665, 0.62934895],\n",
       "        [0.24393638, 0.33000244],\n",
       "        [0.41451292, 0.33000244],\n",
       "        [0.50745527, 0.33000244],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.25755467, 0.53255253],\n",
       "        [0.4499006 , 0.59662418],\n",
       "        [0.26172962, 0.4598691 ],\n",
       "        [0.06923459, 0.23813297],\n",
       "        [0.21115665, 0.67723045],\n",
       "        [0.08926441, 0.23685842],\n",
       "        [0.03274354, 0.13616948],\n",
       "        [0.39885686, 0.33000244],\n",
       "        [0.07321074, 0.24068205],\n",
       "        [0.3026839 , 0.70547709],\n",
       "        [0.06610338, 0.13744402],\n",
       "        [0.34875746, 0.6407165 ],\n",
       "        [0.17445328, 0.40682053],\n",
       "        [0.27897614, 0.45194626],\n",
       "        [0.        , 0.        ],\n",
       "        [0.3416004 , 0.59111264],\n",
       "        [0.12206759, 0.33337926],\n",
       "        [0.27967197, 0.41956597],\n",
       "        [0.14299205, 0.27457802],\n",
       "        [0.32569583, 0.52566311],\n",
       "        [0.1361332 , 0.22648984],\n",
       "        [0.11833996, 0.1640372 ],\n",
       "        [0.16515905, 0.34361006],\n",
       "        [0.02731113, 0.0820186 ],\n",
       "        [0.14080517, 0.46744747],\n",
       "        [0.69035785, 0.33000244],\n",
       "        [0.        , 0.        ],\n",
       "        [0.09423459, 0.27430245],\n",
       "        [0.07718688, 0.14832931],\n",
       "        [0.36600398, 0.33000244],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.21115665, 0.59490183],\n",
       "        [0.34030815, 0.47502584],\n",
       "        [0.08832008, 0.28549776],\n",
       "        [0.15402584, 0.26265932],\n",
       "        [0.11540755, 0.22555977],\n",
       "        [0.01553181, 0.07175336],\n",
       "        [0.        , 0.        ],\n",
       "        [0.08797217, 0.25597658],\n",
       "        [0.21115665, 0.63417155],\n",
       "        [0.32435388, 0.33000244],\n",
       "        [0.32992048, 0.33000244],\n",
       "        [0.07311133, 0.13510162],\n",
       "        [0.08026839, 0.19910437],\n",
       "        [0.49801193, 0.68756459],\n",
       "        [0.11814115, 0.19107819],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.43792247, 0.33000244],\n",
       "        [0.47519881, 0.33000244],\n",
       "        [0.07475149, 0.2244919 ],\n",
       "        [0.20989066, 0.33000244],\n",
       "        [0.51093439, 0.33000244],\n",
       "        [0.14557654, 0.33096796],\n",
       "        [0.26197813, 0.46813641],\n",
       "        [0.50149105, 0.33000244],\n",
       "        [0.08757455, 0.20396142],\n",
       "        [0.0750497 , 0.29321392],\n",
       "        [0.27847913, 0.43334482],\n",
       "        [0.05775348, 0.14681364],\n",
       "        [0.        , 0.        ],\n",
       "        [0.26351889, 0.33000244],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.47112326, 0.7006545 ],\n",
       "        [0.08444334, 0.09755425],\n",
       "        [0.3499503 , 0.33000244],\n",
       "        [0.18389662, 0.32349294],\n",
       "        [0.02196322, 0.07612814],\n",
       "        [0.02940855, 0.08832243],\n",
       "        [0.32440358, 0.68343093],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.10318091, 0.21450224],\n",
       "        [0.2027336 , 0.33000244],\n",
       "        [0.19264414, 0.45056838],\n",
       "        [0.46863817, 0.81157423],\n",
       "        [0.06247515, 0.12986566],\n",
       "        [0.15054672, 0.33582501],\n",
       "        [0.42152087, 0.33000244],\n",
       "        [0.19781312, 0.5377196 ],\n",
       "        [0.07097416, 0.22955563],\n",
       "        [0.09249503, 0.22053049],\n",
       "        [0.14274354, 0.16486393],\n",
       "        [0.09547714, 0.29080262],\n",
       "        [0.09637177, 0.19820875],\n",
       "        [0.2139165 , 0.36100586],\n",
       "        [0.43712724, 0.56562177],\n",
       "        [0.2610338 , 0.35308302],\n",
       "        [0.59542744, 0.67654151],\n",
       "        [0.26356859, 0.37754048],\n",
       "        [0.14150099, 0.29514296],\n",
       "        [0.3278827 , 0.37857389],\n",
       "        [0.14885686, 0.27712711],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.46973161, 0.33000244],\n",
       "        [0.30526839, 0.35067172],\n",
       "        [0.08822068, 0.22387186],\n",
       "        [0.06978131, 0.25783672],\n",
       "        [0.2250497 , 0.38890803],\n",
       "        [0.14617296, 0.20261798],\n",
       "        [0.06267396, 0.21822253],\n",
       "        [0.14234592, 0.31598347],\n",
       "        [0.13732604, 0.28556666],\n",
       "        [0.10472167, 0.19297279],\n",
       "        [0.32892644, 0.33000244],\n",
       "        [0.27584493, 0.45401309],\n",
       "        [0.62872763, 0.7413021 ],\n",
       "        [0.27654076, 0.48777127],\n",
       "        [0.22877734, 0.37444023],\n",
       "        [0.13692843, 0.30141233],\n",
       "        [0.12117296, 0.28704788],\n",
       "        [0.        , 0.        ],\n",
       "        [0.38578529, 0.51291767],\n",
       "        [0.01194831, 0.0358939 ],\n",
       "        [0.01620775, 0.08611781],\n",
       "        [0.25357853, 0.38787461],\n",
       "        [0.14184891, 0.41956597],\n",
       "        [0.04426938, 0.16241819],\n",
       "        [0.15044732, 0.377196  ],\n",
       "        [0.21545726, 0.38580779],\n",
       "        [0.21115665, 0.33000244],\n",
       "        [0.15248509, 0.21136755],\n",
       "        [0.13156064, 0.36858422],\n",
       "        [0.32584493, 0.25514984],\n",
       "        [0.63916501, 0.77402687]]), 0.8461538461538461)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_b():\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X_train_imputed = imp.fit_transform(X_train_missing)  \n",
    "    X_test_imputed = imp.transform(X_test_missing)\n",
    "    \n",
    "    clf = LogisticRegression().fit(X_train_imputed, y_train) \n",
    "    test_score = clf.score(X_test_imputed,y_test)\n",
    "    \n",
    "    return (X_train_imputed, X_test_imputed, test_score)\n",
    "\n",
    "answer_three_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) (5 points) \n",
    "\n",
    "Please impute the missing values using `SimpleImputer` with `strategy='mean'` and `add_indicator=True`. Then fit a LogisticRegression with default hyper-parameters, and return the imputed data and the test score.\n",
    "\n",
    "\n",
    "*This function should a return a tuple of two arrays and one number: `(X_train_imputed, X_test_imputed, test_score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huan Zhao\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.11332008, 0.31484671, 0.        , 0.        ],\n",
       "        [0.03768887, 0.06965208, 0.        , 0.        ],\n",
       "        [0.48663022, 0.56079917, 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.06988072, 0.17402687, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.20402584, 0.33251808, 0.        , 0.        ]]),\n",
       " array([[0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.1027833 , 0.20685498, 0.        , 0.        ],\n",
       "        [0.13180915, 0.28301757, 0.        , 0.        ],\n",
       "        [0.06829026, 0.25938684, 0.        , 0.        ],\n",
       "        [0.14527833, 0.17268343, 0.        , 0.        ],\n",
       "        [0.12892644, 0.29424733, 0.        , 0.        ],\n",
       "        [0.14801193, 0.33000244, 0.        , 1.        ],\n",
       "        [0.05705765, 0.19776094, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.10124254, 0.14033758, 0.        , 0.        ],\n",
       "        [0.31312127, 0.38167413, 0.        , 0.        ],\n",
       "        [0.14696819, 0.23926972, 0.        , 0.        ],\n",
       "        [0.02548708, 0.08832243, 0.        , 0.        ],\n",
       "        [0.13772366, 0.29059594, 0.        , 0.        ],\n",
       "        [0.35109344, 0.48122632, 0.        , 0.        ],\n",
       "        [0.21115665, 0.62934895, 1.        , 0.        ],\n",
       "        [0.24393638, 0.33000244, 0.        , 1.        ],\n",
       "        [0.41451292, 0.33000244, 0.        , 1.        ],\n",
       "        [0.50745527, 0.33000244, 0.        , 1.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.25755467, 0.53255253, 0.        , 0.        ],\n",
       "        [0.4499006 , 0.59662418, 0.        , 0.        ],\n",
       "        [0.26172962, 0.4598691 , 0.        , 0.        ],\n",
       "        [0.06923459, 0.23813297, 0.        , 0.        ],\n",
       "        [0.21115665, 0.67723045, 1.        , 0.        ],\n",
       "        [0.08926441, 0.23685842, 0.        , 0.        ],\n",
       "        [0.03274354, 0.13616948, 0.        , 0.        ],\n",
       "        [0.39885686, 0.33000244, 0.        , 1.        ],\n",
       "        [0.07321074, 0.24068205, 0.        , 0.        ],\n",
       "        [0.3026839 , 0.70547709, 0.        , 0.        ],\n",
       "        [0.06610338, 0.13744402, 0.        , 0.        ],\n",
       "        [0.34875746, 0.6407165 , 0.        , 0.        ],\n",
       "        [0.17445328, 0.40682053, 0.        , 0.        ],\n",
       "        [0.27897614, 0.45194626, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.3416004 , 0.59111264, 0.        , 0.        ],\n",
       "        [0.12206759, 0.33337926, 0.        , 0.        ],\n",
       "        [0.27967197, 0.41956597, 0.        , 0.        ],\n",
       "        [0.14299205, 0.27457802, 0.        , 0.        ],\n",
       "        [0.32569583, 0.52566311, 0.        , 0.        ],\n",
       "        [0.1361332 , 0.22648984, 0.        , 0.        ],\n",
       "        [0.11833996, 0.1640372 , 0.        , 0.        ],\n",
       "        [0.16515905, 0.34361006, 0.        , 0.        ],\n",
       "        [0.02731113, 0.0820186 , 0.        , 0.        ],\n",
       "        [0.14080517, 0.46744747, 0.        , 0.        ],\n",
       "        [0.69035785, 0.33000244, 0.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.09423459, 0.27430245, 0.        , 0.        ],\n",
       "        [0.07718688, 0.14832931, 0.        , 0.        ],\n",
       "        [0.36600398, 0.33000244, 0.        , 1.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.21115665, 0.59490183, 1.        , 0.        ],\n",
       "        [0.34030815, 0.47502584, 0.        , 0.        ],\n",
       "        [0.08832008, 0.28549776, 0.        , 0.        ],\n",
       "        [0.15402584, 0.26265932, 0.        , 0.        ],\n",
       "        [0.11540755, 0.22555977, 0.        , 0.        ],\n",
       "        [0.01553181, 0.07175336, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.08797217, 0.25597658, 0.        , 0.        ],\n",
       "        [0.21115665, 0.63417155, 1.        , 0.        ],\n",
       "        [0.32435388, 0.33000244, 0.        , 1.        ],\n",
       "        [0.32992048, 0.33000244, 0.        , 1.        ],\n",
       "        [0.07311133, 0.13510162, 0.        , 0.        ],\n",
       "        [0.08026839, 0.19910437, 0.        , 0.        ],\n",
       "        [0.49801193, 0.68756459, 0.        , 0.        ],\n",
       "        [0.11814115, 0.19107819, 0.        , 0.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.43792247, 0.33000244, 0.        , 1.        ],\n",
       "        [0.47519881, 0.33000244, 0.        , 1.        ],\n",
       "        [0.07475149, 0.2244919 , 0.        , 0.        ],\n",
       "        [0.20989066, 0.33000244, 0.        , 1.        ],\n",
       "        [0.51093439, 0.33000244, 0.        , 1.        ],\n",
       "        [0.14557654, 0.33096796, 0.        , 0.        ],\n",
       "        [0.26197813, 0.46813641, 0.        , 0.        ],\n",
       "        [0.50149105, 0.33000244, 0.        , 1.        ],\n",
       "        [0.08757455, 0.20396142, 0.        , 0.        ],\n",
       "        [0.0750497 , 0.29321392, 0.        , 0.        ],\n",
       "        [0.27847913, 0.43334482, 0.        , 0.        ],\n",
       "        [0.05775348, 0.14681364, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.26351889, 0.33000244, 0.        , 1.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.47112326, 0.7006545 , 0.        , 0.        ],\n",
       "        [0.08444334, 0.09755425, 0.        , 0.        ],\n",
       "        [0.3499503 , 0.33000244, 0.        , 1.        ],\n",
       "        [0.18389662, 0.32349294, 0.        , 0.        ],\n",
       "        [0.02196322, 0.07612814, 0.        , 0.        ],\n",
       "        [0.02940855, 0.08832243, 0.        , 0.        ],\n",
       "        [0.32440358, 0.68343093, 0.        , 0.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.10318091, 0.21450224, 0.        , 0.        ],\n",
       "        [0.2027336 , 0.33000244, 0.        , 1.        ],\n",
       "        [0.19264414, 0.45056838, 0.        , 0.        ],\n",
       "        [0.46863817, 0.81157423, 0.        , 0.        ],\n",
       "        [0.06247515, 0.12986566, 0.        , 0.        ],\n",
       "        [0.15054672, 0.33582501, 0.        , 0.        ],\n",
       "        [0.42152087, 0.33000244, 0.        , 1.        ],\n",
       "        [0.19781312, 0.5377196 , 0.        , 0.        ],\n",
       "        [0.07097416, 0.22955563, 0.        , 0.        ],\n",
       "        [0.09249503, 0.22053049, 0.        , 0.        ],\n",
       "        [0.14274354, 0.16486393, 0.        , 0.        ],\n",
       "        [0.09547714, 0.29080262, 0.        , 0.        ],\n",
       "        [0.09637177, 0.19820875, 0.        , 0.        ],\n",
       "        [0.2139165 , 0.36100586, 0.        , 0.        ],\n",
       "        [0.43712724, 0.56562177, 0.        , 0.        ],\n",
       "        [0.2610338 , 0.35308302, 0.        , 0.        ],\n",
       "        [0.59542744, 0.67654151, 0.        , 0.        ],\n",
       "        [0.26356859, 0.37754048, 0.        , 0.        ],\n",
       "        [0.14150099, 0.29514296, 0.        , 0.        ],\n",
       "        [0.3278827 , 0.37857389, 0.        , 0.        ],\n",
       "        [0.14885686, 0.27712711, 0.        , 0.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.46973161, 0.33000244, 0.        , 1.        ],\n",
       "        [0.30526839, 0.35067172, 0.        , 0.        ],\n",
       "        [0.08822068, 0.22387186, 0.        , 0.        ],\n",
       "        [0.06978131, 0.25783672, 0.        , 0.        ],\n",
       "        [0.2250497 , 0.38890803, 0.        , 0.        ],\n",
       "        [0.14617296, 0.20261798, 0.        , 0.        ],\n",
       "        [0.06267396, 0.21822253, 0.        , 0.        ],\n",
       "        [0.14234592, 0.31598347, 0.        , 0.        ],\n",
       "        [0.13732604, 0.28556666, 0.        , 0.        ],\n",
       "        [0.10472167, 0.19297279, 0.        , 0.        ],\n",
       "        [0.32892644, 0.33000244, 0.        , 1.        ],\n",
       "        [0.27584493, 0.45401309, 0.        , 0.        ],\n",
       "        [0.62872763, 0.7413021 , 0.        , 0.        ],\n",
       "        [0.27654076, 0.48777127, 0.        , 0.        ],\n",
       "        [0.22877734, 0.37444023, 0.        , 0.        ],\n",
       "        [0.13692843, 0.30141233, 0.        , 0.        ],\n",
       "        [0.12117296, 0.28704788, 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.38578529, 0.51291767, 0.        , 0.        ],\n",
       "        [0.01194831, 0.0358939 , 0.        , 0.        ],\n",
       "        [0.01620775, 0.08611781, 0.        , 0.        ],\n",
       "        [0.25357853, 0.38787461, 0.        , 0.        ],\n",
       "        [0.14184891, 0.41956597, 0.        , 0.        ],\n",
       "        [0.04426938, 0.16241819, 0.        , 0.        ],\n",
       "        [0.15044732, 0.377196  , 0.        , 0.        ],\n",
       "        [0.21545726, 0.38580779, 0.        , 0.        ],\n",
       "        [0.21115665, 0.33000244, 1.        , 1.        ],\n",
       "        [0.15248509, 0.21136755, 0.        , 0.        ],\n",
       "        [0.13156064, 0.36858422, 0.        , 0.        ],\n",
       "        [0.32584493, 0.25514984, 0.        , 0.        ],\n",
       "        [0.63916501, 0.77402687, 0.        , 0.        ]]),\n",
       " 0.9090909090909091)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three_c():\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    X_train_imputed = imp.fit_transform(X_train_missing) \n",
    "    X_test_imputed = imp.transform(X_test_missing)\n",
    "    \n",
    "    clf = LogisticRegression().fit(X_train_imputed, y_train)\n",
    "    test_score=clf.score(X_test_imputed, y_test) \n",
    "\n",
    "    return (X_train_imputed, X_test_imputed, test_score)\n",
    "\n",
    "answer_three_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) (5 points) \n",
    "\n",
    "Why the additional indicators can be helpful when the missing values are missing not at random?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 3(d)\n",
    "\n",
    "The additional Indicators could allow a predictive estimator to account for missingness despite imputation.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f19_lab_2_ans.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
