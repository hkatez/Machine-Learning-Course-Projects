{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWTe9VS3_b11"
   },
   "source": [
    "## SI 670 Applied Machine Learning, Week 5:  Trees, Bagging, Boosting, Calibration. (Due 10/17 11:59pm)\n",
    "\n",
    "For this assignment, question 1 is worth 40 points, and question 2 and 3 are worth 20 points each, for a total of 80 points. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
    "\n",
    "Submit your completed notebook file AND corresponding **HTML** file to the Canvas site.\n",
    "\n",
    "As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates: if you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vIZegu_3GU0"
   },
   "source": [
    "### Put your name here: Huan Zhao\n",
    "\n",
    "### Put your uniquename here: huanzhao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8E6z1mGZ3GU1"
   },
   "source": [
    "### Question 1 (40 points)\n",
    "\n",
    "Please write the answers as well as your derivation process of the following questions. You can use either LaTeX or python code to represent your answer. For example, if you want to present <$x_1^2$>, in the LaTeX format you should write <(dollar sign) x_1^2 (dollar sign)>; in the python code format you should write <\\`x_1\\*\\*2\\`>. See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to represent more mathmatical symbols in LaTeX format.\n",
    "\n",
    "*Note: The whole question 1 does not require coding.*\n",
    "\n",
    "#### (a) (10 points) One-hot encoding\n",
    "\n",
    "If you have a dataset with three data points, and each data point has three features. Among them, X2 and X3 are categorical variables:\n",
    "\n",
    "|    X1\t|  X2 \t|  X3\t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1.3\t|  a \t| a \t|\n",
    "|   0.7 |  b \t| c \t|\n",
    "|   2.1 |  a \t| b     |\n",
    "\n",
    "Please mannually convert this dataset into numerical format with the categorical variables transformed into one-hot encoding. Please keep the order of X1, X2, and X3, and use the alphabetical order for the one-hot encoding.\n",
    "\n",
    "\n",
    "#### (b) (10 points) Calibration\n",
    "Recall the calibration curve has the predicted probability as its x-axis and the true probability as its y-axis. Now you are given a binary classifier and its predicted probability on a test set with 15 data points. The labels of these data points are also given. Please calculate the true probabilities in three bins: \\[0, 0.3), \\[0.3, 0.7), \\[0.7, 1\\]. You could further use these probabilities to draw a calibration curve but it's not required for this question. You only need to give the 3 numbers indicating the true probabilities.\n",
    "|Predicted probability | Label |\n",
    "|----\t               |----   |\n",
    "|   0.40               |   0   |\n",
    "|   0.77               |   1   |\n",
    "|   0.84               |   0   |\n",
    "|   0.68               |   0   |\n",
    "|   0.73               |   1   |\n",
    "|   0.88               |   1   |\n",
    "|   0.69               |   0   |\n",
    "|   0.24               |   0   |\n",
    "|   0.70               |   1   |\n",
    "|   0.41               |   1   |\n",
    "|   0.34               |   1   |\n",
    "|   0.18               |   1   |\n",
    "|   0.31               |   1   |\n",
    "|   0.58               |   1   |\n",
    "|   0.00               |   0   |\n",
    "\n",
    "\n",
    "#### (c) (10 points) Random forests parameters\n",
    "\n",
    "Suppose your current random forest classifier is facing an overfitting situation. Please state whether increase or decrease the following parameters can potentially help you reduce overfitting?\n",
    "\n",
    "(i) `n_estimators`\n",
    "\n",
    "(ii) `max_features`\n",
    "\n",
    "(iii) `max_depth`\n",
    "\n",
    "(iv) `n_jobs`\n",
    "\n",
    "\n",
    "### Question 1 (40 points)\n",
    "\n",
    "Please write the answers as well as your derivation process of the following questions. You can use either LaTeX or python code to represent your answer. For example, if you want to present <$x_1^2$>, in the LaTeX format you should write <(dollar sign) x_1^2 (dollar sign)>; in the python code format you should write <\\`x_1\\*\\*2\\`>. See [here](https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/) for how to represent more mathmatical symbols in LaTeX format.\n",
    "\n",
    "*Note: The whole question 1 does not require coding.*\n",
    "\n",
    "#### (a) (10 points) One-hot encoding\n",
    "\n",
    "If you have a dataset with three data points, and each data point has three features. Among them, X2 and X3 are categorical variables:\n",
    "\n",
    "|    X1\t|  X2 \t|  X3\t|\n",
    "|----\t|----\t|----\t|\n",
    "|   1.3\t|  a \t| a \t|\n",
    "|   0.7 |  b \t| c \t|\n",
    "|   2.1 |  a \t| b     |\n",
    "\n",
    "Please mannually convert this dataset into numerical format with the categorical variables transformed into one-hot encoding. Please keep the order of X1, X2, and X3, and use the alphabetical order for the one-hot encoding.\n",
    "\n",
    "\n",
    "#### (b) (10 points) Calibration\n",
    "Recall the calibration curve has the predicted probability as its x-axis and the true probability as its y-axis. Now you are given a binary classifier and its predicted probability on a test set with 15 data points. The labels of these data points are also given. Please calculate the true probabilities in three bins: \\[0, 0.3), \\[0.3, 0.7), \\[0.7, 1\\]. You could further use these probabilities to draw a calibration curve but it's not required for this question. You only need to give the 3 numbers indicating the true probabilities.\n",
    "\n",
    "|Predicted probability | Label |\n",
    "|----\t               |----   |\n",
    "|   0.40               |   0   |\n",
    "|   0.77               |   1   |\n",
    "|   0.84               |   0   |\n",
    "|   0.68               |   0   |\n",
    "|   0.73               |   1   |\n",
    "|   0.88               |   1   |\n",
    "|   0.69               |   0   |\n",
    "|   0.24               |   0   |\n",
    "|   0.70               |   1   |\n",
    "|   0.41               |   1   |\n",
    "|   0.34               |   1   |\n",
    "|   0.18               |   1   |\n",
    "|   0.31               |   1   |\n",
    "|   0.58               |   1   |\n",
    "|   0.00               |   0   |\n",
    "\n",
    "\n",
    "#### (c) (10 points) Random forests parameters\n",
    "\n",
    "Suppose your current random forest classifier is facing an overfitting situation. Please state whether increase or decrease the following parameters can potentially help you reduce overfitting?\n",
    "\n",
    "(i) `n_estimators`\n",
    "\n",
    "(ii) `max_features`\n",
    "\n",
    "(iii) `max_depth`\n",
    "\n",
    "(iv) `n_jobs`\n",
    "\n",
    "\n",
    "#### (d) (10 points) Polynomial kernel and polynomial feature expansion\n",
    "\n",
    "For a data point with two-dimentional features, i.e., $x=[x_1, x_2]^T$, recall that the the result of applying the `sklearn.preprocessing.PolynomialFeatures` (with parameter `d=3`) to $x$ is\n",
    "$$[1, x_1, x_2, x_1^2, x_2^2, x_1x_2, x_1^3, x_2^3, x_1^2x_2, x_1x_2^2]^T.$$\n",
    "\n",
    "Now you are given a slightly different polynomial feature transformation method, which transforms $x$ to\n",
    "$$\\hat{x} = [1, \\sqrt{3}x_1, \\sqrt{3}x_2, \\sqrt{3}x_1^2, \\sqrt{3}x_2^2, \\sqrt{6}x_1x_2, x_1^3, x_2^3, \\sqrt{3}x_1^2x_2, \\sqrt{3}x_1x_2^2]^T.$$\n",
    "\n",
    "Also recall that a polynomial kernel is defined as $k(x, y) = (1 + x^Ty)^d$. Suppose you have two data points with two-dimentional features, $$x=[x_1, x_2]^T, y = [y_1, y_2]^T,$$ and $d=3$, prove that $k(x, y) = \\hat{x}^T \\hat{y}$, where $\\hat{x}$ and $\\hat{y}$ are the the results of applying the new feature transformation method mentioned above to $x$ and $y$, respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    X1\t|  X2_a \t|  X2_b\t|   X2_c \t|  X3_a \t|  X3_b \t|   X3_c \t|\n",
    "|----\t|----\t|----\t|----\t|----\t|----\t|----\t|\t\n",
    "|   1.3\t|   1\t|  0\t|  0 \t|  1 \t|  0 \t|   0\t| \n",
    "|   0.7 |   0\t| 1 \t|   0\t|   0\t|   0\t|   1\t| \n",
    "|   2.1 |   1\t|  0    |  0 \t|   0\t|   1\t|   0\t| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.3%<br>\n",
    "57.14%<br>\n",
    "80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help reduce overfitting, I would test decreasing (i) n_estimators, (ii) max_features, (iii) max_depth. \n",
    "<br>(iv) n_jobs wouldn't help reduce overfitting either by increasing or decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "$\\hat{x}^T = [1, \\sqrt{3}x_1, \\sqrt{3}x_2, \\sqrt{3}x_1^2, \\sqrt{3}x_2^2, \\sqrt{6}x_1x_2, x_1^3, x_2^3, \\sqrt{3}x_1^2x_2, \\sqrt{3}x_1x_2^2]$\n",
    "\n",
    "$\\hat{y} = [1, \\sqrt{3}y_1, \\sqrt{3}y_2, \\sqrt{3}y_1^2, \\sqrt{3}y_2^2, \\sqrt{6}y_1y_2, y_1^3, y_2^3, \\sqrt{3}y_1^2y_2, \\sqrt{3}y_1y_2^2]^T$  \n",
    "$LHS=\\hat{x}^T\\hat{y} = 1 + 3x_1y_1 + 3x_2y_2 + 3x_1^2y_1^2+3x_2^2y_2^2+6x_1x_2y_1y_2+x_1^3y_1^3+x_2^3y_2^3+3x_1^2x_2y_1^2y_2+3x_1x_2^2y_1y_2^2  $  \n",
    "\n",
    "Since\n",
    "$x=[x_1, x_2]^T, y = [y_1, y_2]^T$  \n",
    "\n",
    "$RHS=k(x, y)$\n",
    "$= (1 + x^Ty)^d$  \n",
    "$=(1+[x_1,x_2][y_1,y_2]^T)^3$  \n",
    "$=(1+x_1y_1+x_2y_2)^3$  \n",
    "$= 1+(x_1y_1)^3+(x_2y_2)^3+3[x_1y_1+x_2y_2+(x_1y_1)^2+(x_1y_1)^2x_2y_2+(x_2y_2)^2+(x_2y_2)^2x_1y_1]+6x_1y_1x_2y_2$  \n",
    "$=1+x_1^3y_1^3+x_2^3y_2^3+3x_1y_1+3x_2y_2+3x_1^2y_1^2+3x_1^2y_1^2x_2y_2+3x_2^2y_2^2+3x_2^2y_2^2x_1y_1+6x_1y_1x_2y_2$\n",
    "$=LHS$\n",
    "\n",
    "so $k(x, y) = \\hat{x}^T \\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9Qlj8Dm3GU5"
   },
   "source": [
    "### Question 2 (20 points) Cross-validation for very small datasets.\n",
    "\n",
    "In our lecture about data leakage, we have talked about a simple strategy to help you avoid data leakage: hold out a test set before you do anything. However, this strategy is not applicable when you have a very small dataset. For example, if you got a dataset with total number of samples as 60, if you use the default train_test_split, you will get a test set with 15 samples. In this case, the evaluation score based on merely 15 samples could be very unreliable and we cannot trust it.\n",
    "\n",
    "To make the evaluation more reliable, people usually use cross-validation to do the evaluation. That is, split the train and test set multiple times and calculate the average of test scores. Here, however, we need to pay additional attention to potential data leakage. One important thing is to make sure all the hyper-parameter search is done within each fold. \n",
    "\n",
    "The general recipe is as follows,\n",
    "* Split the whole dataset into k folds\n",
    "* For i from 1 to k\n",
    "    - Take the i-th fold as the test set and other folds as the training set\n",
    "    - Both tune the hyper-parameters and learn the model on the training set\n",
    "    - Calculate the test score on the test set(the defalut score method in Ridge regression)\n",
    "* Report the average of the test scores\n",
    "\n",
    "In this question, please do 4-fold cross-validation on the subset of boston dataset by using KFold. You can use default train_test_split to get train and validation data. Train a Ridge regression model and tune the hyper-parameter alpha within each fold. We have selected first 60 samples from the dataset and given 'alpha_list' for you to tune. Finally, you need to return the mean value for test scores in all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3674484089866703"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.datasets import load_boston\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import KFold\n",
    "    import numpy as np\n",
    "    \n",
    "    X,y = load_boston(return_X_y=True)\n",
    "    X = X[:60,:]\n",
    "    y = y[:60]\n",
    "    alpha = [0.001,0.01,0.1,1,10]\n",
    "    \n",
    "    score_r2test = []\n",
    "    kf = KFold(n_splits=4, shuffle=False)\n",
    "    for train_index , test_index in kf.split(X):\n",
    "        X_train_fold, y_train_fold, X_test_fold, y_test_fold = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_fold, y_train_fold, random_state = 0)\n",
    "        score_r2=[]\n",
    "        \n",
    "        for i in alpha:\n",
    "            lin_ridge = Ridge(alpha=i).fit(X_test, y_test)\n",
    "            score_r2.append(lin_ridge.score(X_test, y_test))\n",
    "        \n",
    "        best_alpha = alpha[np.argmax(score_r2)]  \n",
    "        model = Ridge(alpha = best_alpha).fit(X_train_fold, y_train_fold)\n",
    "        score_r2test.append(model.score(X_test_fold, y_test_fold))\n",
    "        mean_test_score = np.mean(score_r2test)\n",
    "    \n",
    "    return mean_test_score\n",
    "    \n",
    "answer_two()    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMn9p67w3GU8"
   },
   "source": [
    "### Question 3 (20 points)\n",
    "\n",
    "Download the dataset from [here](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) (Use the german.data). The last column is the prediction target and the remaining columns are features. First transform the categorical features into one-hot encoding. Then train a random forest classifier. You should return the train accuracy and test accuracy. Please use random_state = 0 for both train_test_split and randomforest classifier. \n",
    "\n",
    "*Hint1: The columns of categorical features are 0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19.*\n",
    "\n",
    "*Hint2: You may have problem using `OneHotEncoder` to handle string values and numerical values at the same time. You can transform the string columns frist and then concatenate with the numberical features.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlbJeDTS3GU8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huan Zhao\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.984, 0.72)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    german = pd.read_table('german.data',sep = ' ',header=None)\n",
    "    X = []\n",
    "    y = german[20]\n",
    "\n",
    "    for i in range(len(german)):\n",
    "        categorical = []\n",
    "        for c in [0, 2, 3, 5, 6, 8, 9, 11, 13, 14, 16, 18, 19]:\n",
    "            categorical.append(german.loc[i][c])\n",
    "        X.append(categorical)    \n",
    "        \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    train_categorical = enc.fit_transform(X).toarray()\n",
    "    X_all = []\n",
    "    for i in range(len(german)):\n",
    "        numerical = []\n",
    "        for n in [1, 4, 7, 10, 12, 15, 17]:\n",
    "            numerical.append(german.loc[i][n])\n",
    "        a = np.append(train_categorical[i], np.asarray(numerical))\n",
    "        X_all.append(a)\n",
    "         \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size = 0.25, random_state=0)\n",
    "    clf = RandomForestClassifier(random_state=0).fit(X_train,y_train)\n",
    "    train_score = clf.score(X_train,y_train)\n",
    "    test_score = clf.score(X_test,y_test)\n",
    "    \n",
    "    return train_score, test_score\n",
    "answer_three()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "si670f19_hw_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
